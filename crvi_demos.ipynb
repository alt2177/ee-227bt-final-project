{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "8RJpRuCLo9"
      },
      "source": [
        "# CRVI Demos\n",
        "(https://colab.research.google.com/github/cvxpy/cvxpy/blob/master/examples/machine_learning/ridge_regression.ipynb#scrollTo=ZkX0i-nBqJIJ)[original notebook]\n",
        "(https://github.com/zotroneneis/machine_learning_basics/blob/master/bayesian_linear_regression.ipynb)[bayesian linear regression example]\n",
        "\n",
        "In this notebook, we demonstrate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Gdrbs3SGyw"
      },
      "source": [
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "8ocqu4OeiF"
      },
      "source": [
        "### Writing the objective function\n",
        "\n",
        "We can decompose the **objective function** as the sum of a **least squares loss function** and an $\\ell_2$ **regularizer**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "GvBNb916TJ"
      },
      "source": [
        "def loss_fn(X, Y, beta):\n",
        "    return cp.pnorm(cp.matmul(X, beta) - Y, p=2)**2\n",
        "\n",
        "def regularizer(beta):\n",
        "    return cp.pnorm(beta, p=2)**2\n",
        "\n",
        "def objective_fn(X, Y, beta, lambd):\n",
        "    return loss_fn(X, Y, beta) + lambd * regularizer(beta)\n",
        "\n",
        "def mse(X, Y, beta):\n",
        "    return (1.0 / X.shape[0]) * loss_fn(X, Y, beta).value"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "20eVfZLQQx"
      },
      "source": [
        "### Generating data\n",
        "Because ridge regression encourages the parameter estimates to be small, and as such tends to lead to models with **less variance** than those fit with vanilla linear regression. We generate a small dataset that will illustrate this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "5xlpNwrBup"
      },
      "source": [
        "def generate_data(m=100, n=20, sigma=5):\n",
        "    \"Generates data matrix X and observations Y.\"\n",
        "    np.random.seed(1)\n",
        "    beta_star = np.random.randn(n)\n",
        "    # Generate an ill-conditioned data matrix\n",
        "    X = np.random.randn(m, n)\n",
        "    # Corrupt the observations with additive Gaussian noise\n",
        "    Y = X.dot(beta_star) + np.random.normal(0, sigma, size=m)\n",
        "    return X, Y\n",
        "\n",
        "m = 100\n",
        "n = 20\n",
        "sigma = 5\n",
        "\n",
        "X, Y = generate_data(m, n, sigma)\n",
        "X_train = X[:50, :]\n",
        "Y_train = Y[:50]\n",
        "X_test = X[50:, :]\n",
        "Y_test = Y[50:]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "x8f4aq7Mxg"
      },
      "source": [
        "### Fitting the model\n",
        "\n",
        "All we need to do to fit the model is create a CVXPY problem where the objective is to minimize the the objective function defined above. We make $\\lambda$ a CVXPY parameter, so that we can use a single CVXPY problem to obtain estimates for many values of $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "MGA1UulYCe"
      },
      "source": [
        "beta = cp.Variable(n)\n",
        "lambd = cp.Parameter(nonneg=True)\n",
        "problem = cp.Problem(cp.Minimize(objective_fn(X_train, Y_train, beta, lambd)))\n",
        "\n",
        "lambd_values = np.logspace(-2, 3, 50)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "beta_values = []\n",
        "for v in lambd_values:\n",
        "    lambd.value = v\n",
        "    problem.solve()\n",
        "    train_errors.append(mse(X_train, Y_train, beta))\n",
        "    test_errors.append(mse(X_test, Y_test, beta))\n",
        "    beta_values.append(beta.value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "2iFQkXiIMO"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Notice that, up to a point, penalizing the size of the parameters reduces test error at the cost of increasing the training error, trading off higher bias for lower variance; in other words, this indicates that, for our example, a properly tuned ridge regression **generalizes better** than a least squares linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "f3MOhUQV75"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "def plot_train_test_errors(train_errors, test_errors, lambd_values):\n",
        "    plt.plot(lambd_values, train_errors, label=\"Train error\")\n",
        "    plt.plot(lambd_values, test_errors, label=\"Test error\")\n",
        "    plt.xscale(\"log\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
        "    plt.title(\"Mean Squared Error (MSE)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_train_test_errors(train_errors, test_errors, lambd_values)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "teplRskDhe"
      },
      "source": [
        "### Regularization path\n",
        "As expected, increasing $\\lambda$ drives the parameters towards $0$. In a real-world example, those parameters that approach zero slower than others might correspond to the more **informative** features. It is in this sense that ridge regression can be considered **model selection.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ORgMa1oroy"
      },
      "source": [
        "def plot_regularization_path(lambd_values, beta_values):\n",
        "    num_coeffs = len(beta_values[0])\n",
        "    for i in range(num_coeffs):\n",
        "        plt.plot(lambd_values, [wi[i] for wi in beta_values])\n",
        "    plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
        "    plt.xscale(\"log\")\n",
        "    plt.title(\"Regularization Path\")\n",
        "    plt.show()\n",
        "\n",
        "plot_regularization_path(lambd_values, beta_values)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}